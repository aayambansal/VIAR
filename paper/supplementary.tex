% ECCV 2026 Submission — Supplementary Material — LNCS/Springer Format
\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}

% Custom colors
\definecolor{neglectcolor}{RGB}{220,50,47}
\definecolor{viarcolor}{RGB}{38,139,210}

% Custom commands
\newcommand{\method}{VIAR}
\newcommand{\visFrac}{\textit{vis\_frac}}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\etal}{\textit{et al.}}
\usepackage{xspace}

\begin{document}

\title{VIAR: Vision-Informed Attention Rebalancing\\for Training-Free Visual Grounding in VLMs\\--- Supplementary Material ---}

\titlerunning{VIAR --- Supplementary Material}

\author{Anonymous ECCV 2026 Submission}
\authorrunning{Anonymous}
\institute{Anonymous Institution}

\maketitle

\setcounter{section}{0}
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thetable}{\Alph{section}\arabic{table}}
\renewcommand{\thefigure}{\Alph{section}\arabic{figure}}

This supplementary material provides additional experimental results, ablation studies, and analysis details referenced in the main paper. We organize the content as follows: GQA results (Section~A), HallusionBench results (Section~B), calibration details (Section~C), mechanistic verification (Section~D), decomposed attention analysis (Section~E), head-level analysis (Section~F), cross-prompt consistency (Section~G), per-sample correlation analysis (Section~H), threshold-shift analysis (Section~I), ablation studies (Section~J), addressing potential concerns (Section~K), and qualitative analysis (Section~L).

%==============================================================================
% SECTION A: GQA Results
%==============================================================================
\section{GQA Results}
\label{sec:supp_gqa}
\setcounter{table}{0}
\setcounter{figure}{0}

On GQA~\cite{hudson2019gqa} (500 samples), \method{} achieves 58.6\% exact-match accuracy compared to the baseline's 58.8\%. The negligible difference (within noise) indicates that \method{} does not help on open-ended visual question answering where the model must generate free-form text. This is expected: the intervention increases the relative weight of visual tokens in the attention computation, which aids discrimination in binary and multiple-choice settings but does not meaningfully change the generation dynamics for open-ended responses.

%==============================================================================
% SECTION B: HallusionBench Results
%==============================================================================
\section{HallusionBench Results}
\label{sec:supp_hallusionbench}
\setcounter{table}{0}
\setcounter{figure}{0}

Table~\ref{tab:hallusionbench} reports results on HallusionBench~\cite{guan2024hallusionbench} (951 image-based samples). The baseline achieves 51.7\% accuracy (near chance), reflecting the difficulty of this benchmark for LLaVA-1.5-7B. \method{} achieves 50.8\%, a marginal decrease. The yes-ratio shifts from 52.0\% (approximately balanced) to 59.6\%, indicating that \method{} induces overcorrection on this benchmark where the baseline is already approximately calibrated. This result is informative: \method{}'s benefit is specific to settings where the baseline exhibits a conservative (``no'') bias. When the baseline is already near-calibrated, the additional visual attention boost pushes yes-ratio past the optimal point. This reinforces the view that \method{} is a diagnostic tool, not a universal performance enhancer.

\begin{table}[t]
    \centering
    \caption{\textbf{HallusionBench results} (951 samples). Both methods are near chance on this challenging benchmark. \method{} overcorrects yes-ratio when the baseline is already approximately calibrated.}
    \label{tab:hallusionbench}
    \begin{tabular}{lccccc}
        \toprule
        Method & Accuracy & F1 & Precision & Recall & Yes Ratio \\
        \midrule
        Baseline & \textbf{51.7\%} & 48.7\% & 44.1\% & 54.4\% & 52.0\% \\
        \method{} ($b{=}2.0$, L8--16) & 50.8\% & 51.7\% & 44.1\% & 62.3\% & 59.6\% \\
        \bottomrule
    \end{tabular}
\end{table}

%==============================================================================
% SECTION C: Calibration Details
%==============================================================================
\section{Calibration Details}
\label{sec:supp_calibration}
\setcounter{table}{0}
\setcounter{figure}{0}

Beyond yes-ratio, we evaluate calibration using Expected Calibration Error (ECE, 10 equal-width bins) and Brier score computed from the model's softmax probabilities over the yes/no tokens. The Brier score improves slightly from 0.123 (baseline) to 0.122 (\method{}), consistent with more calibrated binary predictions. ECE is comparable between methods (0.043 baseline vs.\ 0.046 \method{}), which is expected: ECE measures the alignment between confidence and accuracy across the confidence spectrum, while \method{}'s primary effect is on the \emph{decision boundary} (shifting borderline cases from ``no'' to ``yes''), not on the confidence distribution shape.

The reliability diagrams (Figure~\ref{fig:calibration}) show that \method{} redistributes samples from the high-confidence ``no'' bin into moderate-confidence bins, consistent with increased uncertainty on previously over-confident negative predictions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{../figures/fig9_calibration_ece.pdf}
    \caption{\textbf{Reliability diagrams} for baseline and \method{} on POPE (500 samples). Each bar shows the fraction of positive outcomes per confidence bin; numbers above bars indicate sample counts. The dashed diagonal represents perfect calibration. \method{} shifts samples from high-confidence bins into moderate-confidence bins, reflecting increased uncertainty on borderline cases.}
    \label{fig:calibration}
\end{figure}

%==============================================================================
% SECTION D: Mechanistic Verification
%==============================================================================
\section{Mechanistic Verification}
\label{sec:supp_mechanistic}
\setcounter{table}{0}
\setcounter{figure}{0}

To confirm that \method{} operates as intended, we measure the change in visual attention fraction ($\Delta \visFrac{}$) at every layer when the intervention is applied to layers 8--16 (Figure~\ref{fig:mechanistic}).

The results reveal three properties. First, target layers show a consistent increase of $+0.003$ to $+0.005$ in visual attention fraction, confirming that the bias successfully redirects attention toward visual tokens. Second, non-target layers (0--7 and 18--30) show $\Delta \visFrac{} \approx 0$, indicating no unintended attention spillover. Third, layer~17 (immediately after the target range) shows a slight compensatory decrease of approximately $-0.005$, suggesting local redistribution. Notably, layer~31 shows $\Delta \visFrac{} = 0.000$ when \method{} targets layers 8--16, confirming that the intervention's effects are spatially localized.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{../figures/fig3_mechanistic.pdf}
    \caption{\textbf{Mechanistic verification.} Change in visual attention fraction ($\Delta$\visFrac{}) per layer when \method{} is applied to layers 8--16. The intervention increases visual attention within the target range (+0.003 to +0.005) with no spillover to non-target layers and a slight compensatory decrease at layer 17.}
    \label{fig:mechanistic}
\end{figure}

%==============================================================================
% SECTION E: Decomposed Attention Analysis
%==============================================================================
\section{Decomposed Attention Analysis}
\label{sec:supp_decomposed}
\setcounter{table}{0}
\setcounter{figure}{0}

A potential concern with the aggregate $\visFrac{}$ metric is that it includes visual-query$\to$visual-key attention, which is structurally close to 1.0 in causal decoders (visual tokens, appearing first, have no text tokens to attend to). To address this, we compute the decomposed text$\to$vis metric across all 32 layers on 200 POPE samples.

Figure~\ref{fig:decomposed} (a) shows that the U-shaped pattern persists clearly in the text$\to$vis fraction alone. The neglect zone layers (8--16) show depressed text$\to$vis attention (mean 0.839, minimum 0.827 at layer~15) compared to early layers (mean 0.886 for layers 2--7) and late layers (mean 0.911 for layers 17--30). Layer~31 drops further to 0.808, confirming the final-layer anomaly. As expected, visual-query$\to$text-key attention is effectively zero across all layers (causal mask constraint), and visual-query$\to$visual-key attention is uniformly~1.0.

Figure~\ref{fig:decomposed} (b) shows per-head variability: the standard deviation of visual attention fraction across the 32 attention heads peaks at layer~31 (0.009) and in the neglect zone (0.004--0.006), indicating that the neglect pattern is not uniform across heads but rather reflects a mixture of visually attentive and visually neglectful heads. Attention entropy also peaks in the neglect zone, suggesting more diffuse (less focused) attention distributions in these layers.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{../figures/fig7_decomposed_attention.pdf}
    \caption{\textbf{Decomposed attention analysis} (200 POPE samples). (a) The text$\to$vis fraction confirms the U-shaped neglect zone independently of structural causal-mask effects. The shaded region marks layers 8--16. (b) Per-head variability (std of vis\_frac across 32 heads) and attention entropy both peak in the neglect zone, indicating heterogeneous and diffuse attention patterns.}
    \label{fig:decomposed}
\end{figure}

%==============================================================================
% SECTION F: Head-Level Analysis
%==============================================================================
\section{Head-Level Analysis}
\label{sec:supp_head}
\setcounter{table}{0}
\setcounter{figure}{0}

To determine whether the neglect zone reflects uniform depression across attention heads or is driven by a subset of ``visual neglect heads,'' we compute the full $32 \times 32$ (layer $\times$ head) text$\to$vis attention matrix on 150 POPE samples (Figure~\ref{fig:head_heatmap}).

The analysis reveals several findings. First, within the neglect zone (layers 8--16), visual attention is depressed relatively uniformly across heads: the inter-head standard deviation is 0.037, compared to 0.118 globally, indicating that the neglect is a \emph{layer-level} phenomenon rather than a head-specific one. Second, the most extreme per-head variation occurs at layers 0--1 and layer 31. Layer~1 is anomalously low overall (mean text$\to$vis = 0.552), and layer~31 exhibits the highest head-to-head variance ($\sigma = 0.202$), suggesting a mixture of functionally distinct heads at the final layer. Third, ``visual specialist'' heads (text$\to$vis $> \mu + \sigma$) are concentrated in early and late layers (layers 3, 5--7, 20--25), while ``visual neglect'' heads (text$\to$vis $< \mu - \sigma$) cluster in layers 0--1, consistent with the overall U-shape.

The uniformity of neglect within the zone has implications for intervention design: since the depression is not driven by a small number of outlier heads, the uniform additive bias is well-matched to the phenomenon. Head-selective interventions would be unnecessary and potentially counterproductive.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{../figures/fig10_head_level_heatmap.pdf}
    \caption{\textbf{Head-level attention analysis} (150 POPE samples). (a) Full $32 \times 32$ text$\to$vis fraction matrix. The neglect zone (dashed red, layers 8--16) shows uniformly depressed visual attention across heads, while layer~31 (purple) exhibits high inter-head variance. (b) Layer means with head-level standard deviation bars.}
    \label{fig:head_heatmap}
\end{figure}

%==============================================================================
% SECTION G: Cross-Prompt Consistency
%==============================================================================
\section{Cross-Prompt Consistency}
\label{sec:supp_cross_prompt}
\setcounter{table}{0}
\setcounter{figure}{0}

To test whether the visual neglect zone is a stable model property or a prompt-dependent artifact, we measure the text$\to$vis attention fraction on three distinct prompt types using the same model: (1) POPE yes/no questions, (2) MMStar multiple-choice questions, and (3) open-ended captioning (``Describe this image briefly.''), each evaluated on 100 samples.

Figure~\ref{fig:cross_prompt} shows that all three prompt types exhibit the same U-shaped pattern with the layer-31 crash. The neglect zone is most pronounced for MMStar (mean text$\to$vis = 0.767 in layers 8--16), moderate for POPE (0.839), and mildest for captioning (0.889). This ordering is interpretable: MMStar's longer, more complex prompts shift more text-query attention toward text tokens, amplifying the relative visual neglect. The critical observation is that the \emph{shape} of the curve, the location and relative depth of the neglect zone, is consistent across all prompt types, confirming that visual neglect is a structural model property rather than a prompt artifact.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{../figures/fig8_cross_prompt_consistency.pdf}
    \caption{\textbf{Cross-prompt consistency of the visual neglect zone.} The text$\to$vis attention fraction follows the same U-shaped pattern across three distinct prompt types: POPE (yes/no), MMStar (multiple-choice), and open captioning. The neglect zone location (layers 8--16) and layer-31 anomaly are invariant to prompt structure.}
    \label{fig:cross_prompt}
\end{figure}

%==============================================================================
% SECTION H: Per-Sample Correlation Analysis
%==============================================================================
\section{Per-Sample Correlation Analysis}
\label{sec:supp_persample}
\setcounter{table}{0}
\setcounter{figure}{0}

The preceding analyses establish the neglect zone as a model-level structural regularity. A natural question is whether the degree of visual neglect varies across individual samples and, if so, whether samples with deeper neglect are more likely to hallucinate. We address this through a per-sample correlation analysis.

For 500 POPE samples, we compute the per-sample text$\to$vis attention depth in the neglect zone (L8--16), defined as the mean text$\to$vis attention fraction across these layers for each individual sample. We then correlate this per-sample neglect depth with prediction correctness (binary: correct/incorrect).

\paragraph{Aggregate neglect depth does not predict individual hallucinations.} The overall Pearson correlation between neglect-zone attention depth and correctness is $r = -0.023$ ($p = 0.615$), indicating no significant per-sample relationship (Figure~\ref{fig:persample}). Cohen's $d$ for the difference in neglect depth between correct and incorrect predictions is $-0.061$, a very small effect. This result is interpretable: the neglect zone is a structural property of the model's computation, not a sample-varying phenomenon.

\paragraph{Transition layers show significant per-sample effects.} A more fine-grained per-layer analysis reveals a nuanced picture. While most individual layers show no significant correlation with correctness, the layers at the \emph{boundary} of the neglect zone exhibit significant per-sample effects (Figure~\ref{fig:persample}a): layer~14 ($r = -0.154$, $p = 0.0005$), layer~15 ($r = -0.142$, $p = 0.0015$), layer~16 ($r = -0.093$, $p = 0.037$), and layer~17 ($r = -0.140$, $p = 0.0017$). Additionally, layer~30 shows a significant correlation ($r = -0.119$, $p = 0.0076$). All significant correlations are negative, meaning that lower text$\to$vis attention at these specific layers is associated with incorrect predictions.

\paragraph{Interpretation.} The concentration of significant per-sample effects at layers 14--17, the transition from the neglect zone to the late-layer recovery region, has a clear mechanistic interpretation. The neglect zone is a model-level structural regularity: all samples experience reduced visual attention in layers 8--16. However, individual samples differ in how effectively the model \emph{exits} the neglect zone and re-engages visual information. Layers 14--17 represent this critical transition, and samples that fail to adequately re-engage visual tokens at these layers are more likely to produce incorrect predictions. This finding bridges the model-level and sample-level perspectives: the neglect zone is a structural regularity, but individual-sample hallucination vulnerability manifests at the zone's boundary.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{../figures/fig15_persample_correlation.pdf}
    \caption{\textbf{Per-sample neglect-hallucination correlation} (500 POPE samples, LLaVA-1.5-7B). (a)~Per-layer Pearson correlation between text$\to$vis attention and prediction correctness. Significant negative correlations ($*$: $p{<}0.05$, $**$: $p{<}0.01$, $***$: $p{<}0.001$) cluster at the neglect zone boundary (layers 14--17). (b)~Violin plot of neglect-zone aggregate attention depth for correct vs.\ incorrect predictions (Cohen's $d{=}-0.061$).}
    \label{fig:persample}
\end{figure}

%==============================================================================
% SECTION I: Threshold-Shift Analysis
%==============================================================================
\section{Threshold-Shift Analysis}
\label{sec:supp_threshold}
\setcounter{table}{0}
\setcounter{figure}{0}

A natural question is whether \method{}'s effect reduces to simple threshold tuning: does adding a constant to attention masks at visual positions do anything beyond biasing the model's yes/no decision boundary? To address this, we compare \method{} against a \emph{logit bias baseline} on the full POPE dataset (9{,}000 samples). The logit bias baseline adds a scalar bias $\beta$ directly to the ``yes'' token logit before the argmax decision, sweeping $\beta \in [0.0, 6.0]$ in increments of 0.25. This creates a yes-ratio vs.\ accuracy curve that represents the \emph{best possible performance achievable by pure threshold tuning} at each yes-ratio.

Figure~\ref{fig:threshold} (a) shows the result. At \method{}'s achieved yes-ratio of 40.4\%, the logit bias baseline achieves 85.3\% accuracy, compared to \method{}'s 84.8\%. The logit bias curve peaks at 86.6\% (bias = 1.25, yes-ratio $\approx$ 48.8\%). In short, \method{} does not outperform threshold tuning at any matched yes-ratio. Its effect is largely equivalent to shifting the decision boundary.

We report this honestly as it constrains the interpretation of \method{}: the attention-level intervention does not induce a qualitatively different decision process from logit-level manipulation. However, this finding does \emph{not} undermine the diagnostic contribution. The key observation is that the \emph{direction} of the shift (toward increased ``yes'' responses) is correctly predicted by the neglect zone analysis: layers 8--16 under-weight visual tokens, and boosting their attention weight produces the same behavioral signature as directly biasing toward positive recognition. The neglect zone thus identifies the \emph{mechanism} underlying the model's conservative bias, even though the downstream behavioral correction is achievable through simpler means.

Figure~\ref{fig:threshold} (b) shows the per-sample logit attribution: \method{} increases yes-propensity (mean $\Delta = +0.235$) with a slight bias toward low-confidence samples (mean $\Delta = 0.282$ for $|\Delta_{\text{logit}}| < 2$ vs.\ $0.211$ for medium-confidence), though the correlation is weak ($r = -0.060$).

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{../figures/fig11_threshold_comparison.pdf}
    \caption{\textbf{Threshold-shift analysis} (9{,}000 POPE samples). (a) \method{}'s accuracy at its achieved yes-ratio falls on or below the logit bias baseline curve, indicating that the behavioral effect is equivalent to threshold tuning. The key diagnostic insight is that the direction of the shift is predicted by the neglect zone analysis. (b) Per-sample logit attribution by baseline confidence level.}
    \label{fig:threshold}
\end{figure}

%==============================================================================
% SECTION J: Ablation Studies
%==============================================================================
\section{Ablation Studies}
\label{sec:supp_ablations}
\setcounter{table}{0}
\setcounter{figure}{0}

\subsection{Adaptive Scaling Alternatives}

We compare five strategies for computing per-layer bias: uniform, linear (from the main paper, Eq.~4), quadratic, binary (below-median only), inverse-rank, and entropy-based. Results on a 200-sample MMStar subset are reported in Table~\ref{tab:adaptive}. Linear, quadratic, and binary strategies all achieve 50.5\%, while entropy-based is worst at 47.0\%. We select the linear strategy as the simplest principled approach.

\begin{table}[t]
    \centering
    \caption{\textbf{Adaptive scaling alternatives} on a 200-sample MMStar subset. Linear, quadratic, and binary perform equally; entropy-based is worst. We adopt the linear strategy for its simplicity.}
    \label{tab:adaptive}
    \begin{tabular}{lc}
        \toprule
        Scaling Strategy & MMStar Accuracy \\
        \midrule
        Baseline (no intervention) & 48.0\% \\
        Uniform (L8--16) & 49.5\% \\
        Linear (Eq.~4 in main paper) & \textbf{50.5\%} \\
        Quadratic & \textbf{50.5\%} \\
        Binary (below-median) & \textbf{50.5\%} \\
        Inverse-rank & 48.5\% \\
        Entropy-based & 47.0\% \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Bias Magnitude Sweep}

Figure~\ref{fig:ablations} (b) shows the effect of bias magnitude $b$ on POPE and MMStar (200 samples each). Both benchmarks exhibit an inverted-U relationship: too little bias has negligible effect, while excessive bias ($b \geq 5$) collapses attention onto visual tokens and degrades performance (72.0\% on POPE). The optimal bias is $b = 2.0$ for POPE and $b = 1.0$ for MMStar. The sharper decline on POPE (binary classification) compared to MMStar (multiple choice) suggests that binary decisions are more sensitive to attention distribution shifts.

\subsection{Layer Ablation}

We test eight layer configurations on POPE and MMStar (200 samples each). Applying \method{} to the full neglect zone (layers 8--16) performs best on POPE (84.5\%), while the adaptive variant performs best on MMStar (50.5\%). Applying to all 32 layers or restricting to only early or late layers performs worse, confirming that the intervention is effective specifically in the neglect zone (Table~\ref{tab:layer_ablation}).

\begin{table}[t]
    \centering
    \caption{\textbf{Layer ablation} on 200-sample subsets. The neglect zone configuration (L8--16) is best for POPE; adaptive is best for MMStar. Intervening on all layers or non-neglect layers is harmful.}
    \label{tab:layer_ablation}
    \begin{tabular}{lcc}
        \toprule
        Layer Configuration & POPE Acc. & MMStar Acc. \\
        \midrule
        Baseline (no intervention) & 82.5\% & 48.0\% \\
        All layers (L0--31) & 78.0\% & 46.5\% \\
        Early only (L0--7) & 81.0\% & 47.5\% \\
        Late only (L17--31) & 82.0\% & 47.0\% \\
        Neglect zone (L8--16) & \textbf{84.5\%} & 49.5\% \\
        Adaptive (Eq.~4 in main paper) & 83.5\% & \textbf{50.5\%} \\
        Layer 31 only & 82.5\% & 48.0\% \\
        Neglect + Layer 31 & 84.0\% & 49.0\% \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/fig4_adaptive.pdf}
        \caption{Adaptive scaling strategies}
        \label{fig:adaptive}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/fig6_bias_sweep.pdf}
        \caption{Bias magnitude sweep}
        \label{fig:bias_sweep}
    \end{subfigure}
    \caption{\textbf{Ablation studies.} (a) Comparison of adaptive scaling strategies on MMStar (200 samples). Linear, quadratic, and binary achieve equivalent performance. (b) Bias magnitude sweep on POPE and MMStar (200 samples each). An inverted-U relationship shows optimal bias at $b{=}2.0$ for POPE and $b{=}1.0$ for MMStar, with performance collapse at $b \geq 5$.}
    \label{fig:ablations}
\end{figure}

%==============================================================================
% SECTION K: Addressing Potential Concerns
%==============================================================================
\section{Addressing Potential Concerns}
\label{sec:supp_concerns}
\setcounter{table}{0}
\setcounter{figure}{0}

We address several specific questions that arise from our analysis.

\paragraph{How is attention fraction computed during inference?} The visual attention fraction (both aggregate and decomposed) is computed from a single forward pass on the full input prompt (system tokens + visual tokens + question tokens). No generation tokens are included; the metric reflects how the model distributes attention when encoding the input, not during autoregressive generation. This ensures the measurement is deterministic and independent of generated content.

\paragraph{Does the U-shape persist across different tasks/prompts?} Yes. Section~G demonstrates that the text$\to$vis U-shape and layer-31 anomaly are consistent across three distinct prompt types (binary yes/no, multiple choice, and open captioning), confirming the neglect zone is a model property rather than a prompt artifact.

\paragraph{Can you provide a text-query-only version of the metric?} Yes. Section~E introduces the decomposed metric, which restricts attention analysis to text-query$\to$visual-key pairs, explicitly excluding the structurally inflated visual-query$\to$visual-key attention. The U-shape persists in this decomposition.

\paragraph{Do you see ECE/Brier improvements, not just yes-ratio shift?} The Brier score improves from 0.123 to 0.122 (Section~C), a modest but directionally consistent improvement. ECE is comparable between methods (0.043 vs.\ 0.046). The dominant calibration effect is the yes-ratio correction, which is the most interpretable metric for a balanced binary benchmark.

\paragraph{Is the neglect driven by specific ``neglect heads''?} No. Section~F presents a full $32 \times 32$ head-level analysis. Within the neglect zone (layers 8--16), the inter-head standard deviation is 0.037, compared to 0.118 globally. The neglect is a layer-level phenomenon, not a head-specific one. This justifies the uniform additive bias: head-selective interventions are unnecessary.

\paragraph{Does the neglect zone generalize to non-LLaVA architectures?} The U-shaped neglect zone generalizes to other direct-projection architectures: LLaVA-NeXT-Mistral-7B (Mistral backbone) shows a closely correlated profile ($r{=}0.70$ with LLaVA-Vicuna). However, it does not generalize to Q-Former-based (InstructBLIP) or alternative architectures (Qwen2-VL), which show qualitatively different attention profiles (main paper, Section~4.6). This specificity strengthens rather than weakens the finding: the neglect zone is tied to the direct-projection design, not an artifact of the metric.

%==============================================================================
% SECTION L: Qualitative Analysis
%==============================================================================
\section{Qualitative Analysis}
\label{sec:supp_qualitative}
\setcounter{table}{0}
\setcounter{figure}{0}

Per-sample analysis on POPE reveals that \method{} corrects 21 samples that the baseline misclassifies, while introducing 17 new errors, resulting in a net gain of 4. The corrected samples are predominantly false negatives: the baseline says ``no'' for objects that are present (e.g., ``Is there a car in the image?'' with baseline $p_\text{yes} = 0.19$, \method{} $p_\text{yes} = 0.39$). The introduced errors are predominantly false positives: \method{} shifts borderline ``no'' cases past the decision threshold (e.g., ``Is there a motorcycle in the image?'' with baseline $p_\text{yes} = 0.25$, \method{} $p_\text{yes} = 0.41$). This asymmetry is consistent with the calibration finding: \method{} systematically increases the model's propensity to affirm the presence of queried objects, correcting the baseline's conservative bias at the cost of occasional false affirmations.

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{splncs04}
\bibliography{references}

\end{document}
