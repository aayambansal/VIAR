% ECCV 2026 Submission â€” LNCS/Springer Format
\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}

% Custom colors
\definecolor{neglectcolor}{RGB}{220,50,47}
\definecolor{viarcolor}{RGB}{38,139,210}

% Custom commands
\newcommand{\method}{VIAR}
\newcommand{\visFrac}{\textit{vis\_frac}}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\etal}{\textit{et al.}}
\usepackage{xspace}

\begin{document}

\title{VIAR: Vision-Informed Attention Rebalancing\\for Training-Free Visual Grounding in VLMs}

\titlerunning{VIAR: Vision-Informed Attention Rebalancing}

\author{Anonymous ECCV 2026 Submission}
\authorrunning{Anonymous}
\institute{Anonymous Institution}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
We discover and characterize the \emph{visual neglect zone}, a systematic pattern in vision-language models (VLMs) where middle transformer layers allocate disproportionately low attention to visual tokens compared to text tokens. Profiling four architectures (LLaVA-1.5-7B, LLaVA-NeXT-Mistral-7B, InstructBLIP-Vicuna-7B, and Qwen2-VL-7B), we find that visual attention fraction follows a U-shaped curve in direct-projection models: early and late layers engage visual tokens, while a contiguous band of middle layers systematically neglects them. The U-shape is present in both LLaVA variants (Vicuna and Mistral backbones, profile correlation $r{=}0.70$) but absent in Q-Former and alternative architectures, establishing it as a structural property of the direct-projection design. Using a decomposed attention metric that isolates text-query$\to$visual-key attention, we confirm this pattern reflects genuine modality imbalance rather than a measurement artifact. Hidden-state analysis provides definitive mechanistic evidence: comparing model activations on real images versus null (black) images reveals that layer~31's residual update has cosine similarity 0.922 regardless of image content, functioning as a near-pure language prior readout. Independent gradient attribution confirms that visual information relevance decays monotonically across layers, with a steeper-than-baseline 5$\times$ drop through the neglect zone. To probe the causal significance of this pattern, we propose \method{} (Vision-Informed Attention Rebalancing), a simple training-free intervention that adds an additive bias to pre-softmax attention scores at visual token positions in the neglect zone layers. On POPE, \method{} perfectly calibrates yes-ratio from 42.8\% to 50.0\% (matching the balanced ground truth distribution) and improves accuracy from 83.6\% to 84.4\%, with improved Brier score (0.123$\to$0.122); Visual Contrastive Decoding overshoots to 62.2\%. On MMStar, an adaptive variant improves overall accuracy by 1.8\%, concentrated in visually demanding categories. We report that the accuracy improvement is not statistically significant ($p{=}0.272$; Cohen's $h{=}0.007$; post-hoc power 6.8\%), while Bayesian analysis confirms the calibration shift with 99.95\% posterior probability. We frame \method{} as a proof-of-concept for the causal relevance of the neglect zone, not as a reliable performance technique.

\keywords{Vision-Language Models \and Attention Analysis \and Visual Grounding \and Hallucination Mitigation \and Mechanistic Interpretability}
\end{abstract}

%==============================================================================
% 1. INTRODUCTION
%==============================================================================
\section{Introduction}
\label{sec:intro}

Vision-language models (VLMs) that combine a pretrained visual encoder with a large language model have achieved remarkable progress on visual question answering, image captioning, and multimodal reasoning~\cite{liu2024visual,liu2024improved,alayrac2022flamingo,li2023blip2,zhu2024minigpt}. Despite these advances, a persistent failure mode is \emph{object hallucination}, where models generate text describing objects or attributes that are absent from the input image~\cite{li2023evaluating,bai2024hallucination,zhou2024analyzing}. While several post-hoc decoding strategies have been proposed to mitigate hallucination~\cite{leng2024mitigating,huang2024opera,yin2024woodpecker}, the internal mechanisms that cause VLMs to neglect visual information remain poorly understood.

In transformer-based language models, attention analysis has revealed specialized functional roles across layers: early layers handle syntactic processing, middle layers perform semantic composition, and late layers execute task-specific readout~\cite{clark2019what,voita2019analyzing,olsson2022context}. Analogous analyses for multimodal transformers remain sparse, particularly regarding how attention is distributed between visual and textual modalities across the depth of the network.

In this work, we address this gap with a systematic \emph{structural mechanistic analysis} of visual attention allocation in VLMs. Our goal is to characterize a previously undocumented structural regularity in cross-modal attention, validate it through multiple independent methodologies, and establish its causal relevance through a minimal intervention. Our investigation yields the following contributions:

\begin{enumerate}[leftmargin=*,itemsep=2pt]
    \item \textbf{Discovery and multi-method validation of the visual neglect zone.} We profile attention patterns across all 32 layers of four VLM architectures (two direct-projection: LLaVA-1.5-7B with Vicuna backbone and LLaVA-NeXT-Mistral-7B with Mistral backbone; two alternative: InstructBLIP-Vicuna-7B with Q-Former and Qwen2-VL-7B), revealing a U-shaped visual attention curve specific to direct-projection designs (cross-backbone correlation $r{=}0.70$). The U-shape is absent in Q-Former and alternative architectures, providing negative controls. Using a decomposed metric that isolates text-query$\to$visual-key attention (Eq.~\ref{eq:decomposed}), we confirm this pattern is not a structural artifact. Head-level analysis of the full $32 \times 32$ (layer $\times$ head) matrix confirms the neglect is layer-uniform (inter-head $\sigma = 0.037$ within the zone vs.\ 0.118 globally), not driven by individual ``neglect heads.'' We provide three independent lines of mechanistic validation: (i)~hidden-state analysis showing layer~31's residual update is near-identical regardless of image content (cosine similarity 0.922), establishing it as a language prior readout layer; (ii)~gradient attribution showing a monotone decay of visual importance with a steeper-than-baseline 5$\times$ drop through the neglect zone; and (iii)~per-sample correlation analysis revealing that individual hallucination vulnerability manifests specifically at the neglect zone boundary (layers 14--17, $p < 0.002$).

    \item \textbf{A training-free proof-of-concept intervention.} We propose \method{} (Vision-Informed Attention Rebalancing), which registers forward hooks on self-attention modules in the neglect zone layers and adds a constant additive bias to the attention mask at visual token positions before softmax computation. \method{} is compatible with eager, SDPA, and flash attention implementations, requires no additional parameters, and adds negligible computational overhead.

    \item \textbf{Empirical validation with rigorous statistical reporting.} On POPE~\cite{li2023evaluating}, \method{} achieves perfect yes-ratio calibration (50.0\% vs.\ baseline 42.8\%) with improved Brier score (0.123$\to$0.122) and outperforms Visual Contrastive Decoding (VCD)~\cite{leng2024mitigating} on both accuracy and calibration. On MMStar~\cite{chen2024are}, the adaptive variant yields a 1.8\% improvement concentrated in visually demanding categories. We additionally evaluate on HallusionBench~\cite{guan2024hallusionbench}. We report that accuracy improvements are not statistically significant ($p{=}0.272$; Cohen's $h{=}0.007$ on 9K POPE; post-hoc power 6.8\%), while Bayesian analysis confirms the calibration shift with 99.95\% posterior probability. We frame \method{} as a proof-of-concept for the causal relevance of the neglect zone, not as a reliable performance technique.
\end{enumerate}

The cross-architecture comparison in Figure~\ref{fig:cross_arch} illustrates the visual neglect zone in direct-projection models and its absence in alternative architectures. The remainder of this paper is organized as follows. Section~\ref{sec:related} reviews related work. Section~\ref{sec:method} formalizes the visual attention fraction metric, describes the neglect zone finding, and presents the \method{} intervention. Section~\ref{sec:experiments} reports experimental results including cross-architecture analysis, mechanistic validation, and statistical tests. Section~\ref{sec:discussion} interprets the findings and acknowledges limitations. Section~\ref{sec:conclusion} concludes. Additional experiments, ablation studies, and qualitative analyses are provided in the supplementary material.

%==============================================================================
% 2. RELATED WORK
%==============================================================================
\section{Related Work}
\label{sec:related}

\paragraph{Vision-language models.}
The dominant paradigm for building VLMs connects a pretrained visual encoder (typically CLIP ViT~\cite{radford2021learning,dosovitskiy2021image}) to a pretrained large language model through a projection module. LLaVA~\cite{liu2024visual} introduced visual instruction tuning with a linear or MLP projection mapping CLIP features into the language model's embedding space. Subsequent work has scaled visual resolution~\cite{liu2024improved}, explored Q-Former architectures~\cite{li2023blip2}, and incorporated additional visual encoders~\cite{tong2024eyes}. Despite these improvements, the internal dynamics of how the language model backbone processes visual tokens remain underexplored.

\paragraph{Object hallucination in VLMs.}
VLM hallucination has been documented through benchmarks including POPE~\cite{li2023evaluating}, MMStar~\cite{chen2024are}, and HallusionBench~\cite{guan2024hallusionbench}. Proposed mitigations include Visual Contrastive Decoding (VCD)~\cite{leng2024mitigating}, which contrasts outputs from original and distorted visual inputs; OPERA~\cite{huang2024opera}, which penalizes over-reliance on summary tokens; Woodpecker~\cite{yin2024woodpecker}, a post-hoc correction pipeline; and hallucination-augmented contrastive learning~\cite{jiang2024hallucination}. OPERA~\cite{huang2024opera} is closest to our work in identifying attention-level pathologies, but targets a different phenomenon (column-wise attention concentration) and modifies the decoding objective rather than the attention computation. Unlike all of these methods, our primary contribution is diagnostic: we characterize a layer-level pattern of visual attention neglect and use the intervention to demonstrate its causal relevance.

\paragraph{Attention analysis in transformers.}
Clark~\etal{}~\cite{clark2019what} mapped syntactic attention patterns in BERT, while Voita~\etal{}~\cite{voita2019analyzing} identified specialized heads. Abnar and Zuidema~\cite{abnar2020quantifying} proposed attention rollout and attention flow to quantify information propagation. The debate on whether attention provides faithful explanations has been extensively discussed~\cite{jain2019attention,wiegreffe2019attention}. In mechanistic interpretability, work on transformer circuits~\cite{elhage2021mathematical,olsson2022context} has identified functional components such as induction heads. Our work extends this tradition to the multimodal setting, characterizing how attention is distributed between modalities rather than between positions within a single modality.

\paragraph{Mechanistic analysis of VLMs.}
Mechanistic interpretability of language models has progressed rapidly, with methods for locating and editing factual associations~\cite{meng2022locating} and dissecting recall mechanisms~\cite{geva2023dissecting}. However, mechanistic analysis of multimodal models remains nascent. Zhou~\etal{}~\cite{zhou2024analyzing} analyzed hallucination patterns through co-occurrence statistics and attention, and Tong~\etal{}~\cite{tong2024eyes} documented visual shortcomings of multimodal LLMs but focused on benchmark evaluation rather than internal mechanisms. Our work provides, to our knowledge, the first systematic layer-by-layer quantification of visual attention fraction across VLM architectures, extending beyond attention to include hidden-state probing, gradient attribution, and per-sample correlation analyses.

%==============================================================================
% 3. METHOD
%==============================================================================
\section{Method}
\label{sec:method}

We first define the visual attention fraction metric used for our diagnostic analysis (Section~\ref{sec:vis_frac}), then describe the visual neglect zone finding (Section~\ref{sec:neglect_zone}), and finally present the \method{} intervention (Section~\ref{sec:viar}).

\subsection{Visual Attention Fraction}
\label{sec:vis_frac}

Consider a VLM that processes an input sequence consisting of $n_v$ visual tokens followed by $n_t$ text tokens, for a total sequence length of $N = n_v + n_t$. At layer $l$, let $A^{(l)} \in \mathbb{R}^{H \times N \times N}$ denote the attention weight matrix after softmax, where $H$ is the number of attention heads. The visual attention fraction at layer $l$ is defined as the average attention weight allocated to visual token positions, aggregated across all heads and all query positions:
\begin{equation}
    \text{vis\_frac}_l = \frac{1}{H \cdot N} \sum_{h=1}^{H} \sum_{i=1}^{N} \sum_{j=1}^{n_v} A^{(l)}_{h,i,j} \,.
\label{eq:visfrac}
\end{equation}
This metric captures the fraction of total attention that flows toward visual tokens at each layer. A value of $n_v / N$ indicates uniform attention distribution across modalities, while values significantly below this baseline indicate relative visual neglect.

\paragraph{Decomposed attention metric.} The aggregate metric in Eq.~\ref{eq:visfrac} sums over \emph{all} query positions. In a causal decoder, visual tokens (which appear first in the sequence) can only attend to other visual tokens; their contribution to $\visFrac{}$ is therefore structurally inflated and uninformative about how the model processes visual information. To isolate the meaningful signal, we define a \emph{decomposed} visual attention fraction that restricts the query set to text tokens only:
\begin{equation}
    \text{vis\_frac}^{\text{text}\to\text{vis}}_l = \frac{1}{H \cdot n_t} \sum_{h=1}^{H} \sum_{i=n_v+1}^{N} \sum_{j=1}^{n_v} A^{(l)}_{h,i,j} \,.
\label{eq:decomposed}
\end{equation}
This metric answers the question: \emph{when text tokens form their representations, how much do they attend to visual information?} The complementary text$\to$text fraction is $1 - \text{vis\_frac}^{\text{text}\to\text{vis}}_l$, since each text query's attention weights sum to one over all keys it can attend to.

\subsection{The Visual Neglect Zone}
\label{sec:neglect_zone}

We profile the visual attention fraction across all 32 transformer layers for two VLM architectures during inference on 500 POPE samples. In LLaVA-1.5-7B (CLIP ViT-L/14, 336$\times$336, $n_v = 576$), the visual attention fraction follows a distinctive U-shaped pattern. Early layers (0--7) maintain $\visFrac{} \approx 0.30$--$0.35$, indicating substantial visual engagement. A contiguous band of middle layers (8--16) shows systematically depressed visual attention, with $\visFrac{}$ dropping to 0.173--0.210. Late layers (17--30) recover to $\visFrac{} \approx 0.25$--$0.30$. Layer~31, the final transformer layer, exhibits a sharp anomaly: $\visFrac{}$ crashes to 0.141, the lowest value across all layers.

In LLaVA-1.6-Vicuna-7B ($n_v = 2880$ visual tokens from higher-resolution encoding), the same U-shaped pattern is evident despite the substantially different visual-to-text token ratio. The neglect zone is narrower (layers 10--14) with $\visFrac{}$ ranging from 0.845 to 0.870 (relatively depressed given that visual tokens dominate the sequence). The layer-31 anomaly is also present.

The consistency of this pattern across model variants with different visual token counts (576 vs.\ 2880) and different training procedures suggests that the visual neglect zone is a systematic architectural phenomenon. We verify that this pattern persists in the decomposed text$\to$vis metric (Eq.~\ref{eq:decomposed}) in the supplementary material (Supplementary Section~E), confirming it is not an artifact of including structurally inflated visual-query attention.

\subsection{VIAR: Vision-Informed Attention Rebalancing}
\label{sec:viar}

To test whether the neglect zone is causally relevant to model behavior, we propose a minimal inference-time intervention. \method{} registers a \texttt{forward\_pre\_hook} on the \texttt{self\_attn} module of each target layer. The hook modifies the 4D attention mask before softmax computation by adding a constant bias to visual token positions:
\begin{equation}
    \tilde{M}_{:,:,:,1:n_v} = M_{:,:,:,1:n_v} + b \,,
\label{eq:viar}
\end{equation}
where $M \in \mathbb{R}^{B \times 1 \times N \times N}$ is the causal attention mask (with $-\infty$ for masked positions and 0 otherwise), and $b > 0$ is a scalar bias. Since the bias is added before softmax, it multiplicatively increases the attention weight allocated to visual tokens by a factor proportional to $e^b$ relative to the unbiased case.

\paragraph{Target layer selection.} We apply \method{} to layers within the identified neglect zone. For LLaVA-1.5-7B, this corresponds to layers $\mathcal{L} = \{8, 9, \ldots, 16\}$; for LLaVA-1.6-Vicuna-7B, $\mathcal{L} = \{10, 11, \ldots, 14\}$.

\paragraph{Adaptive scaling.} Rather than applying a uniform bias across all target layers, we also consider an adaptive variant where the bias is proportional to the degree of visual neglect at each layer:
\begin{equation}
    b_l = b \cdot \frac{(1 - \visFrac{}_l)}{\max_{l' \in \mathcal{L}} (1 - \visFrac{}_{l'})} \,,
\label{eq:adaptive}
\end{equation}
where $\visFrac{}_l$ is the empirically measured visual attention fraction at layer $l$ (Eq.~\ref{eq:visfrac}), and $b$ is the global bias magnitude. This assigns stronger correction to layers with more severe visual neglect.

\paragraph{Implementation details.} The intervention operates on the attention mask tensor, which is a standard argument to transformer attention implementations. This makes \method{} compatible with PyTorch's eager attention, scaled dot-product attention (SDPA), and flash attention backends. The hook is registered at inference time and requires no model weight modifications, no additional parameters, and adds negligible computational overhead (a single element-wise addition per target layer).

%==============================================================================
% 4. EXPERIMENTS
%==============================================================================
\section{Experiments}
\label{sec:experiments}

We evaluate \method{} across four benchmarks spanning binary VQA (POPE, HallusionBench~\cite{guan2024hallusionbench}), multiple-choice VQA (MMStar), and open-ended VQA (GQA), and compare against Visual Contrastive Decoding (VCD)~\cite{leng2024mitigating}. We additionally validate our diagnostic findings with cross-architecture profiling (Section~\ref{sec:cross_arch}), hidden-state probing (Section~\ref{sec:collapse}), gradient attribution (Section~\ref{sec:gradient}), and statistical analysis (Section~\ref{sec:stats}). All experiments use greedy decoding unless otherwise specified. Additional results on GQA, HallusionBench, decomposed attention, head-level analysis, cross-prompt consistency, per-sample correlation, threshold-shift analysis, and ablation studies are in the supplementary material.

\subsection{Benchmarks and Evaluation}
\label{sec:benchmarks}

\paragraph{POPE~\cite{li2023evaluating}.} A binary yes/no VQA benchmark for evaluating object hallucination. We use the random split with 500 balanced samples (50\% positive, 50\% negative). Metrics: accuracy, F1, precision, recall, and yes-ratio (50\% indicates perfect calibration).

\paragraph{MMStar~\cite{chen2024are}.} A multiple-choice VQA benchmark with 1500 samples spanning six categories: Coarse Perception, Fine-Grained Perception, Instance-Level Reasoning, Logical Reasoning, Mathematics, and Science \& Technology.

\paragraph{Baselines.} We compare against: (1) the unmodified LLaVA-1.5-7B baseline, and (2) VCD with $\alpha = 1.0$~\cite{leng2024mitigating}. We additionally evaluate on HallusionBench~\cite{guan2024hallusionbench} (951 samples) and GQA~\cite{hudson2019gqa} (500 samples); full results are in Supplementary Sections~A--B.

\subsection{Main Results on POPE}
\label{sec:pope_results}

Table~\ref{tab:pope} presents the POPE results. The baseline LLaVA-1.5-7B achieves 83.6\% accuracy with a yes-ratio of 42.8\%, indicating a systematic bias toward ``no'' answers. \method{} with $b = 2.0$ applied to layers 8--16 shifts the yes-ratio to exactly 50.0\%, achieving perfect calibration for this balanced dataset. This correction improves accuracy to 84.4\% and F1 to 84.4\%, with precision and recall becoming equal at 84.4\%.

VCD overshoots in the opposite direction, shifting the yes-ratio to 62.2\% and reducing accuracy to 79.0\%. Combining \method{} with VCD exacerbates this overshooting, pushing the yes-ratio to 70.8\% and accuracy to 75.2\%. These results demonstrate that the two methods are not complementary: VCD already amplifies visual signals through contrastive decoding, and adding \method{}'s attention-level boost creates excessive visual influence.

\begin{table}[t]
    \centering
    \caption{\textbf{POPE results} (500 samples, balanced yes/no). \method{} is the only method that improves both accuracy and calibration. VCD overshoots the yes-ratio. Best results in \textbf{bold}.}
    \label{tab:pope}
    \begin{tabular}{lccccc}
        \toprule
        Method & Accuracy & F1 & Precision & Recall & Yes Ratio \\
        \midrule
        Baseline & 83.6\% & 82.3\% & 89.3\% & 76.4\% & 42.8\% \\
        \textbf{\method{}} ($b{=}2.0$, L8--16) & \textbf{84.4\%} & \textbf{84.4\%} & 84.4\% & \textbf{84.4\%} & \textbf{50.0\%} \\
        VCD ($\alpha{=}1.0$) & 79.0\% & 81.3\% & 73.3\% & 91.2\% & 62.2\% \\
        \method{}+VCD & 75.2\% & 79.5\% & 67.8\% & 96.0\% & 70.8\% \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/fig2_vcd_comparison.pdf}
    \caption{\textbf{Calibration comparison.} Yes-ratio for each method on POPE (500 balanced samples). The dashed line at 50\% indicates perfect calibration. \method{} achieves exactly 50.0\%, while VCD overshoots to 62.2\%.}
    \label{fig:vcd_comparison}
\end{figure}

\subsection{Cross-Model Validation}
\label{sec:cross_model}

To test the prediction that \method{} helps only when visual neglect is severe, we apply it to LLaVA-1.6-Vicuna-7B (which has a narrower neglect zone). With $b = 2.0$ applied to layers 10--14, accuracy on POPE remains at 88.0\%, identical to the baseline. This null result is consistent with the hypothesis: the stronger model already allocates more relative attention to visual tokens in its middle layers, leaving less room for improvement. The null result on LLaVA-1.6 thus serves as a negative control that supports our mechanistic account.

\subsection{MMStar Results}
\label{sec:mmstar_results}

Table~\ref{tab:mmstar} reports MMStar results. The uniform \method{} variant (layers 8--16, $b = 1.0$) improves overall accuracy from 32.3\% to 33.4\% (+1.1\%). The adaptive variant (Eq.~\ref{eq:adaptive}) further improves to 34.1\% (+1.8\%). Improvements are concentrated in visually demanding categories: Instance-Level Reasoning (+4.4\%) and Science \& Technology (+5.6\%). Categories that rely more on linguistic reasoning show minimal change, as expected for an intervention that boosts visual token attention.

\begin{table}[t]
    \centering
    \caption{\textbf{MMStar results} (1500 samples). Improvements concentrate in visually demanding categories. ``Adapt.'' denotes adaptive scaling (Eq.~\ref{eq:adaptive}).}
    \label{tab:mmstar}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lccccccc}
        \toprule
        Method & Overall & Coarse & Fine-Gr. & Instance & Logical & Math & Sci\&Tech \\
        \midrule
        Baseline & 32.3\% & 55.2\% & 28.0\% & 36.4\% & 28.8\% & 26.4\% & 18.8\% \\
        \method{} (L8--16) & 33.4\% & 56.8\% & 28.8\% & 38.8\% & 29.6\% & 25.2\% & 21.2\% \\
        \method{}-Adapt. & \textbf{34.1\%} & \textbf{58.0\%} & 27.6\% & \textbf{40.8\%} & 28.4\% & 25.2\% & \textbf{24.4\%} \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsection{Statistical Significance and Effect Sizes}
\label{sec:stats}

We report rigorous statistical tests for the POPE accuracy improvement. On the initial 500-sample subset, the 95\% bootstrap CI spans $[-1.6\%, +3.4\%]$ with $p = 0.272$. On the \textbf{full 9{,}000-sample POPE dataset}, the accuracy difference narrows to $+0.26\%$ (84.58\% baseline $\to$ 84.83\% \method{}) with tighter bounds: 95\% CI $[-0.20\%, +0.69\%]$, one-sided $p = 0.132$. McNemar's test yields $\chi^2 = 1.207$ ($p = 0.272$). Even with the larger sample size, the accuracy improvement does not reach conventional significance thresholds. We therefore frame \method{} as a proof-of-concept for the causal relevance of the neglect zone, not as a reliable performance technique. The calibration effect, with yes-ratio shifting from 38.0\% toward 40.4\% (partial correction toward balance), is the more robust behavioral signature. On the 500-sample balanced subset (Table~\ref{tab:pope}), the correction reaches exactly 50.0\%.

Table~\ref{tab:stats_merged} consolidates the statistical analysis, including effect sizes and Bayesian posteriors. The accuracy effect is negligible (Cohen's $h = 0.007$ on 9K) with severely insufficient power (6.8\%). Achieving 80\% power at the observed effect size would require approximately 325{,}000 samples. In contrast, Bayesian analysis confirms the calibration shift: the posterior probability that \method{} shifts yes-ratio above baseline is 99.95\%, with the 95\% credible interval $[+0.010, +0.038]$ lying entirely above zero. The Brier score improves slightly from 0.123 to 0.122, consistent with more calibrated predictions. Reliability diagrams and detailed ECE analysis are in Supplementary Section~C.

\begin{table}[t]
    \centering
    \caption{\textbf{Statistical analysis of POPE results.} Accuracy improvement is not significant; the calibration shift is confirmed by Bayesian analysis with 99.95\% posterior probability.}
    \label{tab:stats_merged}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccc}
        \toprule
        & \multicolumn{2}{c}{POPE (500 balanced)} & \multicolumn{2}{c}{POPE (9{,}000 full)} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        Metric & Baseline & \method{} & Baseline & \method{} \\
        \midrule
        Accuracy & 83.6\% & 84.4\% & 84.58\% & 84.83\% \\
        Accuracy $\Delta$ / 95\% CI & \multicolumn{2}{c}{+0.8\% [$-1.6$, $+3.4$]\%} & \multicolumn{2}{c}{+0.26\% [$-0.20$, $+0.69$]\%} \\
        One-sided $p$ / McNemar & \multicolumn{2}{c}{$p{=}0.272$ / $\chi^2{=}0.237$} & \multicolumn{2}{c}{$p{=}0.132$ / $\chi^2{=}1.207$} \\
        Yes-ratio & 42.8\% & \textbf{50.0\%} & 38.0\% & 40.4\% \\
        Brier score & 0.123 & \textbf{0.122} & --- & --- \\
        \midrule
        \multicolumn{5}{l}{\textit{Effect sizes and Bayesian analysis (9K POPE):}} \\
        \midrule
        Cohen's $h$ (accuracy / yes-ratio) & \multicolumn{4}{c}{0.007 / 0.049} \\
        Post-hoc power (accuracy) & \multicolumn{4}{c}{6.8\%; $N$ for 80\% power $\approx$ 325{,}000} \\
        Bayesian $P(\text{VIAR} > \text{base})$: acc.\ / yes-ratio & \multicolumn{4}{c}{67.8\% / 99.95\%} \\
        95\% credible interval: acc.\ / yes-ratio & \multicolumn{4}{c}{$[-0.008, +0.013]$ / $[+0.010, +0.038]$} \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsection{Cross-Architecture Analysis}
\label{sec:cross_arch}

To test whether the visual neglect zone generalizes beyond LLaVA-1.5-7B, we profile the text$\to$vis attention fraction across all 32 layers for four architectures on 200 POPE samples each: (1)~LLaVA-1.5-7B (Vicuna backbone, direct projection, 576 visual tokens), (2)~LLaVA-NeXT-Mistral-7B (Mistral backbone, direct projection), (3)~InstructBLIP-Vicuna-7B (Q-Former, 32 query tokens), and (4)~Qwen2-VL-7B (multi-resolution rotary position embeddings). Figure~\ref{fig:cross_arch} presents the results.

The most striking finding is the strong correspondence between the two direct-projection models despite their different language model backbones. LLaVA-NeXT-Mistral-7B exhibits a clear U-shaped neglect zone spanning layers 10--16, with a mean text$\to$vis fraction of 0.506. Layer~31 shows the same crash behavior, dropping to 0.339. The Pearson correlation between the two 32-layer profiles is $r = 0.699$ ($p < 0.001$). InstructBLIP-Vicuna-7B, which interposes a Q-Former that compresses visual information into 32 learnable query tokens, shows a qualitatively different profile: the characteristic mid-layer depression is absent, and layer~31 is the \emph{highest} attention layer (0.850) rather than the lowest. Qwen2-VL-7B's architecture produces extremely low and flat text$\to$vis values (${\sim}0.0003$), reflecting its use of mrope for cross-modal position encoding rather than raw attention weights.

These results establish the visual neglect zone as a structural property of direct-projection architectures, conserved across language model backbones ($r{=}0.70$) but absent in Q-Former and alternative designs. The Q-Former's intermediate processing appears to mitigate the neglect pattern entirely.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{../figures/fig12_cross_architecture.pdf}
    \caption{\textbf{Cross-architecture attention profiles} (200 POPE samples each). (a)~Direct-projection models (LLaVA-Vicuna and LLaVA-Mistral) both exhibit the U-shaped neglect zone with layer-31 crash ($r{=}0.70$). (b)~InstructBLIP (Q-Former) shows no U-shape. (c)~Normalized overlay. (d)~Pairwise correlation matrix.}
    \label{fig:cross_arch}
\end{figure}

\subsection{Language Prior Collapse: Hidden-State Analysis}
\label{sec:collapse}

To move beyond attention analysis and examine what the model computes at each layer, we conduct a hidden-state probing experiment. We process 200 POPE samples through LLaVA-1.5-7B twice: once with the real image and once with a black (null) image, keeping the text prompt identical. At each layer, we extract hidden states at text-token positions and compute the residual update cosine similarity, which compares only the incremental update added by each layer, isolating the per-layer contribution.

Early layers (L2--7) show moderately image-dependent updates ($\bar{s} = 0.819$). The neglect zone (L8--16) shows intermediate behavior ($\bar{s} = 0.699$). Late layers (L17--30) show the most image-dependent updates ($\bar{s} = 0.655$), consistent with active re-engagement with visual information. Layer~31 is the critical finding: its residual update achieves the highest similarity of any layer ($s = 0.922$), meaning that layer~31's computation is nearly identical regardless of whether the model is processing a real image or a black image. This is definitive evidence that layer~31 functions as a near-pure language prior readout layer.

Additionally, the raw cosine similarity between text-token hidden states (real vs.\ null image) shows a sharp recovery at layer~31 ($s = 0.832$), and visual token norm differences transition to a large positive value at layer~31 (+18.4), indicating decoupled visual-token and text-token processing (Figure~\ref{fig:collapse}). Full analysis details are in Supplementary Section~D.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{../figures/fig13_language_prior_collapse.pdf}
    \caption{\textbf{Language prior collapse analysis} (200 POPE samples, LLaVA-1.5-7B). Hidden states at text-token positions compared between real-image and null-image (black) forward passes. (a)~Raw cosine similarity with sharp recovery at layer~31. (b)~Residual update cosine similarity: layer~31's update is near-identical regardless of image content ($s{=}0.922$). (c)~Visual token norm difference (real~$-$~black) transitions to +18.4 at layer~31.}
    \label{fig:collapse}
\end{figure}

\subsection{Gradient Attribution of Visual Information}
\label{sec:gradient}

To complement forward-pass analyses, we provide backward-pass validation through gradient attribution. For 100 POPE samples, we compute the gradient of the yes-token logit with respect to hidden states at visual-token positions at each layer. Early layers (L2--7) show the highest gradient norms (mean 0.00803). The neglect zone (L8--16) shows a 57\% reduction (mean 0.00346), and late layers (L17--30) show a 90\% reduction (mean 0.00082). Layer~31 has effectively zero gradient signal, consistent with language prior readout.

The key finding is that the gradient norm drops by a factor of approximately 5$\times$ from layer 8 to layer 16, a steeper decline than the baseline decay rate established by the early-to-late gradient envelope. This provides independent gradient-based evidence that visual representations become disproportionately less behaviorally relevant specifically in the neglect zone layers (Figure~\ref{fig:gradient}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/fig14_gradient_attribution.pdf}
    \caption{\textbf{Gradient attribution of visual information} (100 POPE samples, LLaVA-1.5-7B). Gradient norm of the yes-token logit w.r.t.\ hidden states at visual positions. The 5$\times$ drop through the neglect zone (L8--16, shaded) exceeds the baseline decay rate. Layer~31 has zero gradient signal.}
    \label{fig:gradient}
\end{figure}

\paragraph{Additional experiments.} We additionally conduct per-sample neglect-hallucination correlation analysis, finding that aggregate neglect depth does not predict individual hallucinations ($r = -0.023$, $p = 0.615$), but significant correlations emerge at the neglect zone boundary (layers 14--17, $p < 0.002$), suggesting that individual hallucination vulnerability manifests at the transition out of the neglect zone. Full results on GQA, HallusionBench, decomposed attention, head-level analysis, cross-prompt consistency, threshold-shift analysis, ablation studies, and qualitative analysis are in the supplementary material (Sections~A--L).

%==============================================================================
% 5. DISCUSSION
%==============================================================================
\section{Discussion}
\label{sec:discussion}

\paragraph{Interpreting the visual neglect zone.}
The U-shaped visual attention pattern admits a functional interpretation consistent with findings in text-only transformers~\cite{clark2019what,olsson2022context,geva2023dissecting}. Early layers perform initial multimodal alignment. Middle layers shift toward abstract semantic processing, relying increasingly on language model priors and allocating less attention to raw visual features (the neglect zone). Late layers re-engage visual tokens for output generation. The conservation of this U-shape across language model backbones (Vicuna and Mistral, $r{=}0.70$) despite different pretraining data suggests that the neglect zone is an emergent consequence of the direct-projection architecture itself. Its absence in Q-Former-based models, where visual information is pre-processed through cross-attention with learned queries, further supports this interpretation.

\paragraph{Language prior collapse at layer~31.}
Layer~31 exhibits the sharpest visual attention drop in both direct-projection models, yet intervening on layer~31 alone has no measurable effect on performance. The hidden-state analysis provides a definitive explanation: with residual update cosine similarity of 0.922, layer~31 functions as a near-pure language prior readout whose computation is determined almost entirely by language context. The zero gradient signal independently confirms that perturbations to visual representations at this layer have no effect on predictions. The model constructs its final-token representation by integrating visual information through early and late layers, then projecting through a final layer that operates primarily on language-level features.

\paragraph{Calibration versus accuracy.}
The most striking result is the calibration effect on POPE. The baseline model's ``no'' bias (yes-ratio 42.8\%) is corrected to exactly 50.0\% by \method{}, with Bayesian analysis confirming this with 99.95\% posterior probability. The accuracy improvement remains ambiguous (67.8\% posterior probability; credible interval spanning zero). This asymmetry between the definitive calibration effect and the ambiguous accuracy effect is the central empirical finding of the intervention analysis. It connects to the broader calibration literature~\cite{guo2017calibration}: the visual neglect zone appears to cause systematic under-reliance on visual evidence, biasing the model toward conservative predictions.

\paragraph{When does VIAR help?}
The intervention helps when the task requires discriminating based on visual evidence (binary or multiple-choice VQA), the model exhibits substantial visual neglect (LLaVA-1.5), the baseline has a conservative bias, and the bias magnitude is calibrated. \method{} does not help for open-ended generation (GQA), when the model already maintains adequate visual attention (LLaVA-1.6), when the baseline is already calibrated (HallusionBench), or when the bias is applied to non-neglect layers. This pattern of successes and failures provides evidence for the mechanistic account: the neglect zone is a real phenomenon with measurable behavioral consequences.

\paragraph{Limitations.}
Attention weights are an imperfect proxy for information flow~\cite{jain2019attention,wiegreffe2019attention}, though convergence of three independent methodologies mitigates this concern. The accuracy improvement is not statistically significant ($p = 0.272$; Cohen's $h = 0.007$; power 6.8\%). The optimal bias and target layers require per-model calibration. Our cross-architecture coverage is limited to 7B-parameter models; whether the pattern holds for larger models or other direct-projection architectures remains open. The aggregate neglect depth does not predict individual hallucinations, and the intervention shows no improvement on open-ended VQA.

%==============================================================================
% 6. CONCLUSION
%==============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have identified and characterized the visual neglect zone, a systematic structural pattern in direct-projection vision-language models where middle transformer layers allocate disproportionately low attention to visual tokens. Through a multi-method mechanistic analysis spanning four architectures, we establish the following findings. The U-shaped neglect zone is reproducible across direct-projection VLMs with different backbones ($r{=}0.70$) but absent in Q-Former-based and alternative architectures, confirming it as a structural consequence of the direct-projection design. Hidden-state probing reveals that layer~31 functions as a near-pure language prior readout (residual update cosine similarity 0.922). Gradient attribution independently validates the neglect zone through a 5$\times$ steeper-than-baseline decay of visual importance. Per-sample analysis reveals that hallucination vulnerability manifests at the neglect zone boundary (layers 14--17, $p < 0.002$).

The \method{} intervention demonstrates that the neglect zone is causally linked to model calibration: correcting the attention deficit shifts POPE yes-ratio from 42.8\% to 50.0\%, improves Brier score, and improves visually demanding MMStar categories by up to 5.6\%. Bayesian analysis confirms the calibration shift with 99.95\% posterior probability while accuracy improvement remains ambiguous (67.8\% posterior; Cohen's $h{=}0.007$). Rather than claiming a new state-of-the-art method, we offer this work as a structural mechanistic contribution. For model developers, the neglect zone suggests that training procedures should encourage sustained visual grounding in middle layers. For the interpretability community, the convergence of attention-based, hidden-state, and gradient-based analyses demonstrates that multimodal transformers exhibit rich, characterizable internal structure amenable to systematic investigation.

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{splncs04}
\bibliography{references}

\end{document}
