{
  "model": "llava-hf/llava-v1.6-vicuna-7b-hf",
  "num_samples": 37,
  "num_layers": 32,
  "n_vis_tokens": 2880,
  "note": "LLaVA-1.6 uses dynamic resolution (~2880 tokens). OOM on some high-res images with output_attentions=True.",
  "key_finding": "CONFIRMED: Visual neglect zone exists in LLaVA-1.6 too, shifted to layers 10-14. Higher baseline vis_frac (84-94%) because visual tokens dominate sequence length even more. Neglect zone shows vis_frac dip to 84-85%, with layers 0-1 also showing relative neglect at 76-79%.",
  "neglect_zone": [0, 1, 10, 11, 12, 13, 14, 31],
  "by_layer": {
    "0":  {"vis_attn_frac_mean": 0.763, "h_ratio_mean": 0.998},
    "1":  {"vis_attn_frac_mean": 0.793, "h_ratio_mean": 0.990},
    "2":  {"vis_attn_frac_mean": 0.878, "h_ratio_mean": 0.291},
    "3":  {"vis_attn_frac_mean": 0.948, "h_ratio_mean": 0.129},
    "4":  {"vis_attn_frac_mean": 0.939, "h_ratio_mean": 0.135},
    "5":  {"vis_attn_frac_mean": 0.931, "h_ratio_mean": 0.157},
    "6":  {"vis_attn_frac_mean": 0.931, "h_ratio_mean": 0.183},
    "7":  {"vis_attn_frac_mean": 0.907, "h_ratio_mean": 0.242},
    "8":  {"vis_attn_frac_mean": 0.893, "h_ratio_mean": 0.293},
    "9":  {"vis_attn_frac_mean": 0.874, "h_ratio_mean": 0.263},
    "10": {"vis_attn_frac_mean": 0.841, "h_ratio_mean": 0.355},
    "11": {"vis_attn_frac_mean": 0.850, "h_ratio_mean": 0.337},
    "12": {"vis_attn_frac_mean": 0.843, "h_ratio_mean": 0.311},
    "13": {"vis_attn_frac_mean": 0.855, "h_ratio_mean": 0.283},
    "14": {"vis_attn_frac_mean": 0.869, "h_ratio_mean": 0.300},
    "15": {"vis_attn_frac_mean": 0.885, "h_ratio_mean": 0.237},
    "16": {"vis_attn_frac_mean": 0.914, "h_ratio_mean": 0.206},
    "17": {"vis_attn_frac_mean": 0.939, "h_ratio_mean": 0.175},
    "18": {"vis_attn_frac_mean": 0.953, "h_ratio_mean": 0.114},
    "19": {"vis_attn_frac_mean": 0.944, "h_ratio_mean": 0.130},
    "20": {"vis_attn_frac_mean": 0.948, "h_ratio_mean": 0.151},
    "21": {"vis_attn_frac_mean": 0.969, "h_ratio_mean": 0.090},
    "22": {"vis_attn_frac_mean": 0.954, "h_ratio_mean": 0.109},
    "23": {"vis_attn_frac_mean": 0.965, "h_ratio_mean": 0.086},
    "24": {"vis_attn_frac_mean": 0.957, "h_ratio_mean": 0.107},
    "25": {"vis_attn_frac_mean": 0.973, "h_ratio_mean": 0.084},
    "26": {"vis_attn_frac_mean": 0.951, "h_ratio_mean": 0.130},
    "27": {"vis_attn_frac_mean": 0.965, "h_ratio_mean": 0.084},
    "28": {"vis_attn_frac_mean": 0.951, "h_ratio_mean": 0.127},
    "29": {"vis_attn_frac_mean": 0.928, "h_ratio_mean": 0.176},
    "30": {"vis_attn_frac_mean": 0.944, "h_ratio_mean": 0.130},
    "31": {"vis_attn_frac_mean": 0.871, "h_ratio_mean": 0.334}
  },
  "comparison_to_llava15": "The U-shape is confirmed but with key differences: (1) baseline vis_frac is much higher (84-97% vs 17-50%) because LLaVA-1.6 has ~2880 visual tokens vs 576, dominating the sequence. (2) The neglect zone is shifted slightly earlier to layers 10-14. (3) Layer 31 again shows anomalous drop. (4) The h_ratio pattern is similar but inverted scale because the majority token type is visual."
}
